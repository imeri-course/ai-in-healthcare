{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron Demo in Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object is like a dictionary, it contains a description of the data and the features and targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full description: \n",
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry \n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
      "        13 is Radius SE, field 23 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Full description: \") \n",
    "print(cancer['DESCR'])\n",
    "cancer['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "print(cancer.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put data to x and target to y\n",
    "x = cancer['data']\n",
    "y = cancer['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def to_dataframe(x, y, columns):\n",
    "    \"\"\"\n",
    "    Convert data to dataframe, just for preview\n",
    "    \"\"\"\n",
    "    # add data features\n",
    "    df = pd.DataFrame(x, columns=columns)\n",
    "    # add Target\n",
    "    df['target'] = y\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View Data\n",
    "to_dataframe(x, y, cancer['feature_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "Split data into train and test data, to make sure our model not overfit with data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split Dataset to train and testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Use standar scale for processing the data <br>\n",
    "`z = (x - u) / s` <br>\n",
    "where `u` is the mean of the training samples and `s` is the standard deviation of the training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(x_train)\n",
    "# Now apply the transformations to the data:\n",
    "x = scaler.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply the transformations to the data:\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.821092</td>\n",
       "      <td>0.124779</td>\n",
       "      <td>-0.756586</td>\n",
       "      <td>-0.749244</td>\n",
       "      <td>-1.138956</td>\n",
       "      <td>0.252542</td>\n",
       "      <td>0.076317</td>\n",
       "      <td>0.214287</td>\n",
       "      <td>2.811776</td>\n",
       "      <td>-0.089902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.562207</td>\n",
       "      <td>-0.863306</td>\n",
       "      <td>-0.770556</td>\n",
       "      <td>-1.283744</td>\n",
       "      <td>-0.435764</td>\n",
       "      <td>-0.548220</td>\n",
       "      <td>-0.308177</td>\n",
       "      <td>0.098256</td>\n",
       "      <td>-0.763413</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.628444</td>\n",
       "      <td>-0.134756</td>\n",
       "      <td>1.607669</td>\n",
       "      <td>1.606861</td>\n",
       "      <td>0.868242</td>\n",
       "      <td>0.751416</td>\n",
       "      <td>1.790055</td>\n",
       "      <td>1.914555</td>\n",
       "      <td>0.023378</td>\n",
       "      <td>-0.252393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.724525</td>\n",
       "      <td>1.476680</td>\n",
       "      <td>1.605370</td>\n",
       "      <td>0.821104</td>\n",
       "      <td>0.729323</td>\n",
       "      <td>1.556281</td>\n",
       "      <td>1.415644</td>\n",
       "      <td>0.610965</td>\n",
       "      <td>0.306206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.518643</td>\n",
       "      <td>2.403674</td>\n",
       "      <td>-0.595722</td>\n",
       "      <td>-0.531894</td>\n",
       "      <td>-1.363455</td>\n",
       "      <td>-1.319193</td>\n",
       "      <td>-1.119919</td>\n",
       "      <td>-1.281478</td>\n",
       "      <td>-0.417275</td>\n",
       "      <td>-0.465836</td>\n",
       "      <td>...</td>\n",
       "      <td>1.947399</td>\n",
       "      <td>-0.652605</td>\n",
       "      <td>-0.549273</td>\n",
       "      <td>-1.655341</td>\n",
       "      <td>-1.253986</td>\n",
       "      <td>-1.289350</td>\n",
       "      <td>-1.730795</td>\n",
       "      <td>-0.801871</td>\n",
       "      <td>-0.905672</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.559570</td>\n",
       "      <td>-0.191671</td>\n",
       "      <td>1.494630</td>\n",
       "      <td>1.561135</td>\n",
       "      <td>0.616468</td>\n",
       "      <td>0.413228</td>\n",
       "      <td>0.586595</td>\n",
       "      <td>1.125986</td>\n",
       "      <td>0.362898</td>\n",
       "      <td>-0.464459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365326</td>\n",
       "      <td>1.977907</td>\n",
       "      <td>2.344199</td>\n",
       "      <td>1.297510</td>\n",
       "      <td>0.596842</td>\n",
       "      <td>0.570185</td>\n",
       "      <td>1.462994</td>\n",
       "      <td>1.057731</td>\n",
       "      <td>-0.076183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.952852</td>\n",
       "      <td>1.092342</td>\n",
       "      <td>-0.966578</td>\n",
       "      <td>-0.848317</td>\n",
       "      <td>-1.018664</td>\n",
       "      <td>-0.720544</td>\n",
       "      <td>-0.920246</td>\n",
       "      <td>-1.063056</td>\n",
       "      <td>0.576001</td>\n",
       "      <td>0.040918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.893748</td>\n",
       "      <td>-0.689114</td>\n",
       "      <td>-0.646506</td>\n",
       "      <td>-0.521495</td>\n",
       "      <td>-0.565768</td>\n",
       "      <td>-0.992419</td>\n",
       "      <td>-1.232252</td>\n",
       "      <td>0.269708</td>\n",
       "      <td>-0.433435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>-0.737245</td>\n",
       "      <td>-1.978817</td>\n",
       "      <td>-0.756151</td>\n",
       "      <td>-0.703518</td>\n",
       "      <td>-0.735418</td>\n",
       "      <td>-0.838816</td>\n",
       "      <td>-0.943593</td>\n",
       "      <td>-1.043060</td>\n",
       "      <td>0.059497</td>\n",
       "      <td>-0.273049</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.072441</td>\n",
       "      <td>-0.771724</td>\n",
       "      <td>-0.715463</td>\n",
       "      <td>-1.006563</td>\n",
       "      <td>-0.570102</td>\n",
       "      <td>-0.890310</td>\n",
       "      <td>-1.010627</td>\n",
       "      <td>0.815389</td>\n",
       "      <td>-0.536119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>-0.916917</td>\n",
       "      <td>-0.187118</td>\n",
       "      <td>-0.901363</td>\n",
       "      <td>-0.822711</td>\n",
       "      <td>0.273776</td>\n",
       "      <td>-0.560231</td>\n",
       "      <td>-0.482202</td>\n",
       "      <td>-0.496656</td>\n",
       "      <td>-1.193837</td>\n",
       "      <td>0.436131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043364</td>\n",
       "      <td>-0.724386</td>\n",
       "      <td>-0.707072</td>\n",
       "      <td>0.167129</td>\n",
       "      <td>-0.558958</td>\n",
       "      <td>-0.612458</td>\n",
       "      <td>-0.526136</td>\n",
       "      <td>-1.062347</td>\n",
       "      <td>-0.376746</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>-1.295427</td>\n",
       "      <td>-1.427875</td>\n",
       "      <td>-1.173961</td>\n",
       "      <td>-1.119929</td>\n",
       "      <td>2.029200</td>\n",
       "      <td>2.163959</td>\n",
       "      <td>0.412461</td>\n",
       "      <td>0.602415</td>\n",
       "      <td>0.868566</td>\n",
       "      <td>4.512193</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.247081</td>\n",
       "      <td>-1.135577</td>\n",
       "      <td>-0.970314</td>\n",
       "      <td>2.951938</td>\n",
       "      <td>0.690941</td>\n",
       "      <td>0.107099</td>\n",
       "      <td>-0.088842</td>\n",
       "      <td>-0.078143</td>\n",
       "      <td>2.782908</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>-0.617463</td>\n",
       "      <td>-0.266800</td>\n",
       "      <td>-0.670067</td>\n",
       "      <td>-0.616639</td>\n",
       "      <td>-0.970407</td>\n",
       "      <td>-1.066392</td>\n",
       "      <td>-0.865555</td>\n",
       "      <td>-0.920385</td>\n",
       "      <td>0.185914</td>\n",
       "      <td>-0.259278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.721851</td>\n",
       "      <td>-0.696230</td>\n",
       "      <td>-0.607102</td>\n",
       "      <td>-0.781353</td>\n",
       "      <td>-1.025364</td>\n",
       "      <td>-0.848172</td>\n",
       "      <td>-0.769145</td>\n",
       "      <td>-0.182004</td>\n",
       "      <td>-0.550024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>-0.524632</td>\n",
       "      <td>0.689380</td>\n",
       "      <td>-0.562680</td>\n",
       "      <td>-0.528540</td>\n",
       "      <td>-1.018664</td>\n",
       "      <td>-0.982499</td>\n",
       "      <td>-0.898834</td>\n",
       "      <td>-0.943940</td>\n",
       "      <td>-0.984346</td>\n",
       "      <td>-0.443803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979956</td>\n",
       "      <td>-0.415605</td>\n",
       "      <td>-0.432519</td>\n",
       "      <td>-0.296285</td>\n",
       "      <td>-0.459907</td>\n",
       "      <td>-0.699706</td>\n",
       "      <td>-0.763188</td>\n",
       "      <td>0.337300</td>\n",
       "      <td>-0.124850</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0      -0.821092      0.124779       -0.756586  -0.749244        -1.138956   \n",
       "1       1.628444     -0.134756        1.607669   1.606861         0.868242   \n",
       "2      -0.518643      2.403674       -0.595722  -0.531894        -1.363455   \n",
       "3       1.559570     -0.191671        1.494630   1.561135         0.616468   \n",
       "4      -0.952852      1.092342       -0.966578  -0.848317        -1.018664   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "450    -0.737245     -1.978817       -0.756151  -0.703518        -0.735418   \n",
       "451    -0.916917     -0.187118       -0.901363  -0.822711         0.273776   \n",
       "452    -1.295427     -1.427875       -1.173961  -1.119929         2.029200   \n",
       "453    -0.617463     -0.266800       -0.670067  -0.616639        -0.970407   \n",
       "454    -0.524632      0.689380       -0.562680  -0.528540        -1.018664   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0            0.252542        0.076317             0.214287       2.811776   \n",
       "1            0.751416        1.790055             1.914555       0.023378   \n",
       "2           -1.319193       -1.119919            -1.281478      -0.417275   \n",
       "3            0.413228        0.586595             1.125986       0.362898   \n",
       "4           -0.720544       -0.920246            -1.063056       0.576001   \n",
       "..                ...             ...                  ...            ...   \n",
       "450         -0.838816       -0.943593            -1.043060       0.059497   \n",
       "451         -0.560231       -0.482202            -0.496656      -1.193837   \n",
       "452          2.163959        0.412461             0.602415       0.868566   \n",
       "453         -1.066392       -0.865555            -0.920385       0.185914   \n",
       "454         -0.982499       -0.898834            -0.943940      -0.984346   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 -0.089902  ...      -0.562207        -0.863306   -0.770556   \n",
       "1                 -0.252393  ...       0.724525         1.476680    1.605370   \n",
       "2                 -0.465836  ...       1.947399        -0.652605   -0.549273   \n",
       "3                 -0.464459  ...       0.365326         1.977907    2.344199   \n",
       "4                  0.040918  ...       0.893748        -0.689114   -0.646506   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "450               -0.273049  ...      -2.072441        -0.771724   -0.715463   \n",
       "451                0.436131  ...      -0.043364        -0.724386   -0.707072   \n",
       "452                4.512193  ...      -1.247081        -1.135577   -0.970314   \n",
       "453               -0.259278  ...      -0.721851        -0.696230   -0.607102   \n",
       "454               -0.443803  ...       0.979956        -0.415605   -0.432519   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0           -1.283744          -0.435764        -0.548220   \n",
       "1            0.821104           0.729323         1.556281   \n",
       "2           -1.655341          -1.253986        -1.289350   \n",
       "3            1.297510           0.596842         0.570185   \n",
       "4           -0.521495          -0.565768        -0.992419   \n",
       "..                ...                ...              ...   \n",
       "450         -1.006563          -0.570102        -0.890310   \n",
       "451          0.167129          -0.558958        -0.612458   \n",
       "452          2.951938           0.690941         0.107099   \n",
       "453         -0.781353          -1.025364        -0.848172   \n",
       "454         -0.296285          -0.459907        -0.699706   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0               -0.308177        0.098256                -0.763413       1  \n",
       "1                1.415644        0.610965                 0.306206       0  \n",
       "2               -1.730795       -0.801871                -0.905672       1  \n",
       "3                1.462994        1.057731                -0.076183       0  \n",
       "4               -1.232252        0.269708                -0.433435       1  \n",
       "..                    ...             ...                      ...     ...  \n",
       "450             -1.010627        0.815389                -0.536119       1  \n",
       "451             -0.526136       -1.062347                -0.376746       1  \n",
       "452             -0.088842       -0.078143                 2.782908       1  \n",
       "453             -0.769145       -0.182004                -0.550024       1  \n",
       "454             -0.763188        0.337300                -0.124850       1  \n",
       "\n",
       "[455 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View Data\n",
    "to_dataframe(x_train, y_train, cancer['feature_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# random seed for reusable training\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset to dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes one epoch is too big to feed to the computer at once we divide it in several smaller batches. <br>\n",
    "`Epoch` = **One Epoch** is when an **ENTIRE dataset** is passed **forward** and **backward** through the neural network **only ONCE**. <br>\n",
    "`Batch Size` = Total number of training examples present **in a single batch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def create_data_loader(x, y, batch_size):\n",
    "    \"\"\"\n",
    "    Function to create data loader, split full data to minibatch\n",
    "    ex(batch_size=2): [1,2,3,4,5,6,7,8] --> [[1,2], [3,4], [5,6], [7,8]]\n",
    "    \"\"\"\n",
    "    # Convert numpy data to torch tensor\n",
    "    x_tensor = torch.Tensor(x)\n",
    "    y_tensor = torch.Tensor(y)\n",
    "    \n",
    "    # Create tensordataset\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    \n",
    "    # Create Dataloader with batch_size\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    return data_loader\n",
    "\n",
    "# Parameter batch_size\n",
    "train_batch_size = 1000\n",
    "test_batch_size = 1000\n",
    "\n",
    "train_loader = create_data_loader(x=x_train, y=y_train, batch_size=train_batch_size)\n",
    "test_loader = create_data_loader(x=x_test, y=y_test, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample data\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([455, 30])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Training Shape\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([455])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target Training \n",
    "example_targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP](https://www.researchgate.net/profile/Nicolas_Yu/publication/316615322/figure/fig1/AS:512951140483072@1499308491041/MLP-with-one-hidden-layer-It-consists-of-three-layers-the-input-layer-the-hidden-layer.png \"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Hidden Layer Network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape=30, output_class=1, hidden_nodes=60):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_shape: Shape of data\n",
    "            output_class: number of output class data\n",
    "            hidden_nodes: number of hidden nodes in each layer \n",
    "        \"\"\"\n",
    "        \n",
    "        # Init\n",
    "        super(Net,self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.output_class = output_class\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        \n",
    "        # linear layer (input_shape -> hidden_nodes)\n",
    "        self.fc1 = nn.Linear(input_shape, hidden_nodes)\n",
    "        \n",
    "        # linear layer (hidden_nodes -> output_class)\n",
    "        self.fc2 = nn.Linear(hidden_nodes, output_class)\n",
    "        \n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.droput = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, self.input_shape)\n",
    "        \n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # add dropout layer\n",
    "        x = self.droput(x)\n",
    "        \n",
    "        # add output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net(input_shape=30, output_class=1)\n",
    "# network = nn.Linear(30, 1)\n",
    "\n",
    "# Use cuda will accelerated by gpu\n",
    "if torch.cuda.is_available():\n",
    "    network.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Optimizers\n",
    "- [Simple theory of Optimizers](https://medium.com/datadriveninvestor/overview-of-different-optimizers-for-neural-networks-e0ed119440c3)\n",
    "- [Torch optim doc](https://pytorch.org/docs/stable/optim.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "# optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion\n",
    "- [A brief overview of loss function in pytorch](https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss() #nn.CrossEntropyLoss() #  nn.MSELoss() # nn.SmoothL1Loss() # nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_epochs = 2000\n",
    "log_interval = 100\n",
    "verbose = False\n",
    "\n",
    "# For storing all losses and accuracy\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(1,n_epochs + 1)]\n",
    "train_acc = []\n",
    "test_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(idx_epoch):\n",
    "    \"\"\"\n",
    "    One Epoch Training Function\n",
    "    \"\"\"\n",
    "    # Set to Train mode on\n",
    "    network.train() \n",
    "    correct = 0\n",
    "    # Start Training\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # Convert data and target to cuda datatype\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        # Main Training Process\n",
    "        optimizer.zero_grad() # Clear all Gradient in Optimizer\n",
    "        output = network(data) # Forward Propagation\n",
    "        loss = criterion(output.view(-1), target) # Calculcate loss between forward result and target\n",
    "        loss.backward() # Backward Propagation\n",
    "        optimizer.step() # Update Gradient in Optimizer\n",
    "        \n",
    "        # calculate the accuracy\n",
    "#         print(target, output)\n",
    "        equals = target.float()  ==  output.t()\n",
    "        correct += (torch.sum(equals).cpu().numpy())\n",
    "        \n",
    "#         pred = output.data.max(1, keepdim=True)[1]\n",
    "#         correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        \n",
    "        # Print all Loss and stores to predefine lists\n",
    "        if batch_idx % log_interval == 0:\n",
    "            if verbose: \n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(idx_epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*train_batch_size) + ((idx_epoch-1)*len(train_loader.dataset)))\n",
    "    train_acc.append((100. * correct / len(train_loader.dataset)).item())\n",
    "    if verbose: \n",
    "        print('Train Accuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(train_loader.dataset), train_acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    \"\"\"\n",
    "    Evaluation Function\n",
    "    \"\"\"\n",
    "    # Set to Train mode of\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Should deactivate all gradient proccess\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Convert data and target to cuda datatype\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "            \n",
    "            # Main Eval Process\n",
    "            output = network(data) # Forward Propagation\n",
    "            \n",
    "            # calculate the accuracy and loss\n",
    "            test_loss += criterion(output.view(-1), target).item()\n",
    "            equals = target.float()  ==  output.t()\n",
    "            correct += (torch.sum(equals).cpu().numpy())\n",
    "#             pred = output.data.max(1, keepdim=True)[1]\n",
    "#             correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    \n",
    "    # Print all Loss and stores to predefine lists\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    test_acc.append((100. * correct / len(test_loader.dataset)).item())\n",
    "    if verbose: \n",
    "        print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), test_acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Process: 100%|██████████| 2000/2000 [00:14<00:00, 139.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "for epoch in tqdm(range(1, n_epochs + 1), desc=\"Training Process\"):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Acc')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(test_counter, train_acc, color='blue')\n",
    "plt.plot(test_counter, test_acc, color='red')\n",
    "plt.legend(['Train Acc', 'Test Acc'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('Acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Home Work !\n",
    "## Rules:\n",
    "- Get accuracy more than **96%** (train and test)\n",
    "- Not overfit with data train\n",
    "- Only use \"Multi-Layer Perceptron\" models,  on your Network\n",
    "- Can change:\n",
    "   - loss function\n",
    "   - learning rate\n",
    "   - momentum\n",
    "   - optimizer\n",
    "   - network configuration (hidden layer, add nn.Linear, etc)\n",
    "   - etc, except above restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "['malignant' 'benign']\n",
      "(114, 30)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, Y = data.data, data.target\n",
    "\n",
    "print(data.feature_names)\n",
    "print(data.target_names)\n",
    "\n",
    "\"\"\"let's preprocess, normalize and create the model\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "class BinaryClassification(torch.nn.Module):\n",
    "  def __init__(self, input_dimension):\n",
    "    super().__init__()\n",
    "    self.linear = torch.nn.Linear(input_dimension, 1)\n",
    "\n",
    "  def forward(self, input_dimension):\n",
    "      return self.linear(input_dimension)\n",
    "    \n",
    "\n",
    "_, input_dimension = X_train.shape\n",
    "\n",
    "model = torch.nn.Linear(input_dimension, 1)\n",
    "\n",
    "\"\"\"train the model\"\"\"\n",
    "\n",
    "def configure_loss_function(): \n",
    "  return torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def configure_optimizer(model):\n",
    "  return torch.optim.Adam(model.parameters())\n",
    "\n",
    "def full_gd(model, criterion, optimizer, X_train, y_train, n_epochs=2000):\n",
    "  train_losses = np.zeros(n_epochs)\n",
    "  test_losses = np.zeros(n_epochs)\n",
    "\n",
    "  for it in range(n_epochs): \n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    outputs_test = model(X_test)\n",
    "    loss_test = criterion(outputs_test, y_test)\n",
    "\n",
    "    train_losses[it] = loss.item()\n",
    "    test_losses[it] = loss_test.item()\n",
    "\n",
    "    if (it + 1) % 50 == 0:\n",
    "      print(f'In this epoch {it+1}/{n_epochs}, Training loss: {loss.item():.4f}, Test loss: {loss_test.item():.4f}')\n",
    "\n",
    "  return train_losses, test_losses\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32)).reshape(-1, 1)\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32)).reshape(-1, 1)\n",
    "\n",
    "criterion = configure_loss_function()\n",
    "optimizer = configure_optimizer(model)\n",
    "train_losses, test_losses = full_gd(model, criterion, optimizer, X_train, y_train)\n",
    "\n",
    "plt.plot(train_losses, label = 'train loss')\n",
    "plt.plot(test_losses, label = 'test loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"evaluate model\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "  p_train = model(X_train)\n",
    "  p_train = (p_train.numpy() > 0)\n",
    "\n",
    "  train_acc = np.mean(y_train.numpy() == p_train)\n",
    "\n",
    "  p_test = model(X_test)\n",
    "  p_test = (p_test.numpy() > 0)\n",
    "  \n",
    "  test_acc = np.mean(y_test.numpy() == p_test)\n",
    "\n",
    "print(train_acc)\n",
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compare-ipmgan",
   "language": "python",
   "name": "compare-ipmgan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
